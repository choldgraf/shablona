{
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "name": "python",
      "mimetype": "text/x-python",
      "version": "3.5.3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "\n# Plotting model stability\n\n\nNext we'll show off another demonstration of model fitting with shablona.\nWe'll generate a bunch of data with varying levels of signal to noise, and then\nshow the stability of the model coefficients for each set of data.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport shablona as sb\nplt.style.use('ggplot')\n\n# Set seed for reproducibility\nnp.random.seed(1337)"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "Generating data\n---------------\n\nWe'll generate some random data for this example.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 100\nw = .3\nnoise_levels = np.linspace(.01, 10, 5)  # These are our SNR levels\nn_boots = 1000  # Number of bootstraps for the coefficients\nx = np.random.randn(N)"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "Fitting the model\n-----------------\n\nNow, we'll loop through the varying levels of noise, and fit several models\nto different subsets of data. This will give us a distribution of model\ncoefficients for each noise level.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "def error_func(x, w):\n    return x * w\n\n\ncoefs = np.zeros([len(noise_levels), n_boots])\nfor ii, n_level in enumerate(noise_levels):\n    # Generate y for this noise level\n    y = w * x + n_level * np.random.randn(N)\n    for jj in range(n_boots):\n        # Pull subsets of data\n        ixs_boot = np.random.randint(0, N, N)\n        x_boot = x[ixs_boot]\n        y_boot = y[ixs_boot]\n        # Fit the model and return the coefs\n        model = sb.Model(error_func)\n        fit = model.fit(x_boot, y_boot, (.5,))\n        coefs[ii, jj] = fit.params[0]"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "Assessing coefficient stability\n-------------------------------\n\nNow we'll assess the stability of the fitted coefficient for varying levels\nof noise. Let's plot the raw values for each noise level, as well as the\n95% confidence interval.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "percentiles = np.percentile(coefs, [2.5, 97.5], axis=1).T\nfig, ax = plt.subplots()\nfor n_level, i_coefs, percs in zip(noise_levels, coefs, percentiles):\n    ax.scatter(np.repeat(n_level, len(i_coefs)), i_coefs)\n    ax.hlines(percs, n_level - .2, n_level + .2, lw=2, color='r', alpha=.6)\nax.set(xlabel='Noise level', ylabel='Boostrapped coefficients',\n       title='Bootstrapped coefficients and 95% CI\\nfor many noise levels')"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}